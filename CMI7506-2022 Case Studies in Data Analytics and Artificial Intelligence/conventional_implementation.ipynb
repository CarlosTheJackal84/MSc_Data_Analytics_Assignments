{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in prepared csv's from EDA\n",
    "training_read = pd.read_csv(\".\\Data\\stellar_training_data.csv\",index_col=0)\n",
    "test_read = pd.read_csv(\".\\Data\\stellar_test_data.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f24198",
   "metadata": {},
   "source": [
    "# Define Functions for Splitting Data & Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381603b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation stretegy; common for all models\n",
    "CV = StratifiedKFold(n_splits=10,\n",
    "                     shuffle=True,\n",
    "                     random_state=42)\n",
    "\n",
    "\n",
    "#dataset sizes\n",
    "DATASET_SIZES = [78052, 15610, 7805, 1561, 781, 156, 78]\n",
    "\n",
    "#dataframe to store results\n",
    "TEST_RESULTS=pd.DataFrame()\n",
    "\n",
    "#number of cores to be used for running script\n",
    "N_CORES=10\n",
    "\n",
    "TEST_RESULTS_FILE_NAME = \"benchmarking_results_12Feb22.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d0381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#short function to append results\n",
    "\n",
    "def append_predictions(clf, results_in, X, algo):\n",
    "    y_preds=clf.predict(X)\n",
    "    y_preds=pd.DataFrame(y_preds,columns=[f\"{algo}_{i}\"])\n",
    "    results_out=pd.concat([results_in,y_preds],axis=1)    \n",
    "    return results_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ae0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_scale_data(train_data, test_data, size):\n",
    "    #sample the dataset\n",
    "    train_data = train_data.groupby(\"class\", group_keys=False).apply(lambda x: x.sample(int(np.rint(size*len(x)/len(train_data)))))\n",
    "\n",
    "    #split the training and test data into predictors and targets\n",
    "    X_train = train_data.drop([\"class\"],axis=1)\n",
    "    y_train = train_data[[\"class\"]].values.ravel()\n",
    "    \n",
    "    X_test = test_data.drop([\"class\"],axis=1)\n",
    "    y_test = test_data[[\"class\"]].values.ravel()\n",
    "          \n",
    "    #scale the predictors\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe682a",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a55ee3",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a20974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNearestNeighbors_modelling():\n",
    "    algo=\"KNN\"\n",
    "    \n",
    "    #define the hyper-parmeter space to grid-search across\n",
    "    knn_param_space={\"algorithm\":[\"ball_tree\",\"kd_tree\",\"brute\"],\n",
    "                    \"leaf_size\":np.arange(1,21,1),\n",
    "                    }\n",
    "    \n",
    "    #create initialise gridsearch and fit\n",
    "    clf_grid = GridSearchCV(KNeighborsClassifier(n_neighbors=3),\n",
    "                           knn_param_space,\n",
    "                           cv=CV,\n",
    "                           n_jobs=N_CORES,\n",
    "                           verbose=3).fit(X_train_scaled,y_train)\n",
    "    \n",
    "    #hand off best parameters to dictionary\n",
    "    clf_best_params = clf_grid.best_params_\n",
    "    algorithm=clf_best_params.get(\"algorithm\")\n",
    "    leaf_size=clf_best_params.get(\"leaf_size\")\n",
    "    \n",
    "    #create the final model with best parameters from gridsearch\n",
    "    clf = KNeighborsClassifier(n_neighbors=3,\n",
    "                              algorithm=algorithm,\n",
    "                              leaf_size=leaf_size).fit(X_train_scaled,y_train)\n",
    "    return clf, algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07938b08",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fcce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression_modelling():\n",
    "    algo=\"LogisticRegression\"\n",
    "    \n",
    "    #define the hyper-parmeter space to grid-search across\n",
    "    lr_param_space={\"C\":np.arange(1,153,2)\n",
    "        \n",
    "    }\n",
    "    \n",
    "    #create initialise gridsearch and fit\n",
    "    clf_grid = GridSearchCV(LogisticRegression(random_state=42,\n",
    "                                               multi_class=\"multinomial\",\n",
    "                                               solver=\"lbfgs\",\n",
    "                                               max_iter=1000),\n",
    "                           lr_param_space,\n",
    "                           cv=CV,\n",
    "                           n_jobs=N_CORES,\n",
    "                           verbose=3).fit(X_train_scaled, y_train)\n",
    "    \n",
    "    \n",
    "    #hand off best parameters to dictionary\n",
    "    clf_best_params = clf_grid.best_params_\n",
    "    C=clf_best_params.get(\"C\")\n",
    "    \n",
    "    #create the final model with best parameters from gridsearch\n",
    "    clf = LogisticRegression(random_state=42,\n",
    "                             C=C,\n",
    "                             multi_class=\"multinomial\",\n",
    "                             solver=\"lbfgs\",\n",
    "                             max_iter=1000).fit(X_train_scaled, y_train)\n",
    "    \n",
    "    return clf, algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f5daa",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31595025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC_modelling():\n",
    "    algo=\"SVC\"\n",
    "    \n",
    "    #define the hyper-parmeter space to grid-search across\n",
    "    svc_param_space={\"C\":np.arange(5,15,5),\n",
    "                \"kernel\":[\"poly\",\"rbf\"]}\n",
    "    \n",
    "    #create initialise gridsearch and fit\n",
    "    clf_grid = GridSearchCV(SVC(random_state=42),\n",
    "                           svc_param_space,\n",
    "                           cv=CV,\n",
    "                           n_jobs=N_CORES,\n",
    "                           verbose=3).fit(X_train_scaled,y_train)\n",
    "    \n",
    "    #hand off best parameters to dictionary\n",
    "    clf_best_params = clf_grid.best_params_\n",
    "    C=clf_best_params.get(\"C\")\n",
    "    kernel=clf_best_params.get(\"kernel\")\n",
    "    \n",
    "    #create the final model with best parameters from gridsearch\n",
    "    clf = SVC(random_state=42,\n",
    "             C=C,\n",
    "             kernel=kernel,\n",
    "             probability=True).fit(X_train_scaled,y_train)\n",
    "\n",
    "    return clf, algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f5c86",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_modelling():\n",
    "    algo=\"MLP\"\n",
    "    \n",
    "   #define the hyper-parmeter space to grid-search across\n",
    "    mlp_param_space={\n",
    "        \"hidden_layer_sizes\":[[50,10],[50,20],[50,30],\n",
    "                              [40,8],[40,16],[40,24],\n",
    "                              [30,6],[30,12],[30,18],\n",
    "                              [20,4],[20,8],[20,12],\n",
    "                              [10,2],[10,4],[10,6]],\n",
    "        \"activation\":[\"identity\",\"logistic\",\"tanh\",\"relu\"],\n",
    "        \"solver\":[\"lbfgs\",\"sgd\",\"adam\"],\n",
    "        \"learning_rate\":[\"constant\",\"adaptive\"]}\n",
    "    \n",
    "    #create initialise gridsearch and fit\n",
    "    clf_grid = GridSearchCV(MLPClassifier(random_state=42,\n",
    "                                         max_iter=1000),\n",
    "                           mlp_param_space,\n",
    "                           cv=CV,\n",
    "                           n_jobs=N_CORES,\n",
    "                           verbose=3).fit(X_train_scaled, y_train)\n",
    "    \n",
    "    \n",
    "    #hand off best parameters to dictionary\n",
    "    clf_best_params = clf_grid.best_params_\n",
    "    hidden_layer_sizes=clf_best_params.get(\"hidden_layer_sizes\")\n",
    "    activation=clf_best_params.get(\"activation\")\n",
    "    solver=clf_best_params.get(\"solver\")\n",
    "    learning_rate=clf_best_params.get(\"learning_rate\")\n",
    "    \n",
    "    #create the final model with best parameters from gridsearch\n",
    "    clf=MLPClassifier(random_state=42,\n",
    "                     max_iter=10000,\n",
    "                     hidden_layer_sizes=hidden_layer_sizes,\n",
    "                     activation=activation,\n",
    "                     solver=solver,\n",
    "                     learning_rate=learning_rate,\n",
    "                     early_stopping=True).fit(X_train_scaled, y_train)\n",
    "    \n",
    "    return clf, algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89af12",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in DATASET_SIZES:\n",
    "    training_results = pd.DataFrame()\n",
    "    #split data\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test=sample_scale_data(training_read, test_read, i)\n",
    "    \n",
    "    #export the splits to .csv's for MatLab\n",
    "    training = np.column_stack([X_train_scaled, y_train])\n",
    "    fname = f\".\\Data\\stellar_training_{i}.csv\"\n",
    "    np.savetxt(fname, training, delimiter=\",\")\n",
    "    \n",
    "    testing = np.column_stack([X_test_scaled, y_test])\n",
    "    fname = f\".\\Data\\stellar_testing_{i}.csv\"\n",
    "    np.savetxt(fname, testing, delimiter=\",\")  \n",
    "    \n",
    "    #KNearestNeighbors Algorithm\n",
    "    knn_clf, algo = KNearestNeighbors_modelling()\n",
    "    training_results = append_predictions(knn_clf, training_results, X_train_scaled, algo)\n",
    "    training_results.to_csv(f\"training_results_{i}.csv\")\n",
    "    TEST_RESULTS = append_predictions(knn_clf, TEST_RESULTS, X_test_scaled, algo)\n",
    "    TEST_RESULTS.to_csv(TEST_RESULTS_FILE_NAME)\n",
    "        \n",
    "    #LogisticRegression Algorithm\n",
    "    lr_clf, algo = LogisticRegression_modelling()\n",
    "    training_results = append_predictions(lr_clf, training_results, X_train_scaled, algo)\n",
    "    training_results.to_csv(f\"training_results_{i}.csv\")\n",
    "    TEST_RESULTS = append_predictions(lr_clf, TEST_RESULTS, X_test_scaled, algo)\n",
    "    TEST_RESULTS.to_csv(TEST_RESULTS_FILE_NAME)\n",
    "    \n",
    "    #SupportVectorMachine Algorithm\n",
    "    svc_clf, algo = SVC_modelling()\n",
    "    training_results = append_predictions(svc_clf, training_results, X_train_scaled, algo)\n",
    "    training_results.to_csv(f\"training_results_{i}.csv\")\n",
    "    TEST_RESULTS = append_predictions(svc_clf, TEST_RESULTS, X_test_scaled, algo)\n",
    "    TEST_RESULTS.to_csv(TEST_RESULTS_FILE_NAME)\n",
    "    \n",
    "    #Neural Network Algorithm\n",
    "    mlp_clf, algo = MLP_modelling()\n",
    "    training_results = append_predictions(mlp_clf, training_results, X_train_scaled, algo)\n",
    "    training_results.to_csv(f\"training_results_{i}.csv\")\n",
    "    TEST_RESULTS = append_predictions(mlp_clf, TEST_RESULTS, X_test_scaled, algo)\n",
    "    TEST_RESULTS.to_csv(TEST_RESULTS_FILE_NAME)\n",
    "    \n",
    "    y_train = pd.DataFrame(y_train,columns=[\"y_train\"])\n",
    "    training_results = pd.concat([y_train,training_results],axis=1)\n",
    "    training_results.to_csv(f\"training_results_{i}.csv\")\n",
    "    \n",
    "y_test = pd.DataFrame(y_test, columns=[\"y_test\"])\n",
    "TEST_RESULTS = pd.concat([y_test,TEST_RESULTS],axis=1)\n",
    "TEST_RESULTS.to_csv(TEST_RESULTS_FILE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

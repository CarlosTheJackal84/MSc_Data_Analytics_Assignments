{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries for analysis\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default=\"simple_white\"\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9644860a",
   "metadata": {},
   "source": [
    "# Training Score Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for the training performance calculations to be stored\n",
    "training_results_df = pd.DataFrame(columns=[\"algo\",\"dataset_size\",\"train_score\"])\n",
    "\n",
    "#loop through each training results file\n",
    "for i in [x for x in os.listdir() if \"training\" in x]:\n",
    "    working_df = pd.read_csv(i,index_col=0)\n",
    "    \n",
    "    columns_list = list(working_df.columns)\n",
    "    columns_list.remove(\"y_train\")\n",
    "    \n",
    "    #lopp through each column in the results file\n",
    "    for j in columns_list:\n",
    "        details = j.split(\"_\")\n",
    "        algo = details[0]\n",
    "        size = details[1]        \n",
    "        \n",
    "        y_true = working_df[[\"y_train\"]]\n",
    "        y_pred = working_df[[j]]\n",
    "        \n",
    "        #calculate the accuracy\n",
    "        accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "        \n",
    "        #append the results to the results df\n",
    "        training_results_df = training_results_df.append({\n",
    "            \"algo\":algo,\n",
    "            \"dataset_size\":size,\n",
    "            \"train_score\":accuracy\n",
    "        },ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390cbe04",
   "metadata": {},
   "source": [
    "# Test Score Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ff28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test predictions matrix \n",
    "raw_results = pd.read_csv(\"test_results_consolidated_15Feb22.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all columns\n",
    "columns_list = list(raw_results.columns)\n",
    "\n",
    "#and drop the ground truth column\n",
    "columns_list.remove(\"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for the test performance calculations to be stored\n",
    "test_results_df = pd.DataFrame(columns=[\"algo\",\"dataset_size\",\"test_score\",\"balanced_accuracy_score\",\n",
    "                                       \"f1_score\"])\n",
    "\n",
    "#loop through all columsn of algo and training sample size\n",
    "for i in columns_list:\n",
    "    details = i.split(\"_\")\n",
    "    algo = details[0]\n",
    "    size = details[1]\n",
    "    \n",
    "    #create the ground truth and prediction series\n",
    "    y_true = raw_results[[\"y_test\"]]\n",
    "    y_pred = raw_results[[i]]\n",
    "    \n",
    "    #calculate the key metrics\n",
    "    f_score = f1_score(y_true,y_pred,average=\"weighted\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    #append results to the results dataframe\n",
    "    test_results_df = test_results_df.append({\n",
    "                                            \"algo\":algo,\n",
    "                                            \"dataset_size\":size,\n",
    "                                            \"test_score\":accuracy,\n",
    "                                            \"balanced_accuracy_score\":bal_acc,\n",
    "                                            \"f1_score\":f_score\n",
    "                                            },ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83109e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c43676",
   "metadata": {},
   "source": [
    "# Join DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the training and test results dataframes key'd off algo and training sample size\n",
    "results_df = pd.merge(training_results_df, test_results_df,\n",
    "                      how=\"left\",\n",
    "                      left_on=[\"algo\", \"dataset_size\"],\n",
    "                      right_on=[\"algo\", \"dataset_size\"])\n",
    "\n",
    "#calculate the \"overfit\" metric\n",
    "results_df[\"overfit\"]=results_df[\"train_score\"]-results_df[\"test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b910a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export results to csv\n",
    "results_df.to_csv(\"results_df_17Feb22.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f2bf5",
   "metadata": {},
   "source": [
    "# Create Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d143a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a list of labels to iterate over\n",
    "labels = [\"train_score\",\"test_score\",\"balanced_accuracy_score\",\"f1_score\",\"overfit\"]\n",
    "\n",
    "#create a dictionary of strings to mark up plot axes\n",
    "titles={\"train_score\":\"Training Score [-]\",\n",
    "       \"test_score\":\"Testing Score [-]\",\n",
    "       \"balanced_accuracy_score\":\"Balanced Accuracy Score [-]\",\n",
    "       \"f1_score\":\"F1 Score [-]\",\n",
    "       \"overfit\": \"Train-Test Delta [-]\"\n",
    "       }\n",
    "\n",
    "#loop through different metrics in labels list\n",
    "for i in labels:\n",
    "    #retrieve the assocaited label from the titles dictionary\n",
    "    title=titles.get(i)\n",
    "    \n",
    "    #create the scatter plot of dataset size vs. metric of interest\n",
    "    fig = px.scatter(results_df,\n",
    "                     x=\"dataset_size\",\n",
    "                     y=i,\n",
    "                     color=\"algo\",\n",
    "                     range_x=[10,100000],\n",
    "                     log_x=True,\n",
    "                     width=640,\n",
    "                     height=640)\n",
    "    \n",
    "    #tidy the plot up\n",
    "    fig.update_xaxes(showgrid=True,showline=True, linewidth=1, mirror=True,title=\"Training set size [observations]\")\n",
    "    fig.update_yaxes(showgrid=True,showline=True, linewidth=1, mirror=True,title = title)\n",
    "    fig.update_traces(marker={\"size\": 15,\n",
    "                             \"symbol\": 134,\n",
    "                             \"line_width\": 3})\n",
    "    fig.update_layout(legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        title=\"Algorithm:\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1\n",
    "    ))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e1fe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create scatter plot of training vs. test scores to examine overfitting\n",
    "fig = px.scatter(results_df,\n",
    "                 x=\"train_score\",\n",
    "                 y=\"test_score\",\n",
    "                 color=\"algo\",\n",
    "                 range_x=[0.4,1],\n",
    "                 range_y=[0.4,1],\n",
    "                 width=640,\n",
    "                 height=640,\n",
    "                )\n",
    "\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    line=dict(dash=\"dash\",color=\"black\",width=3),\n",
    "    x0=0.4,x1=1,y0=0.4,y1=1\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    showgrid=True,\n",
    "    showline=True,\n",
    "    linewidth=1,\n",
    "    mirror=True,\n",
    "    title=\"Training Score [-]\"\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    showgrid=True,\n",
    "    showline=True,\n",
    "    linewidth=1,\n",
    "    mirror=True,\n",
    "    title=\"Testing Score [-]\"\n",
    ")\n",
    "\n",
    "fig.update_traces(marker={\"size\": 15,\n",
    "                         \"symbol\": 134,\n",
    "                         \"line_width\": 3})\n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"v\",\n",
    "    yanchor=\"top\",\n",
    "    y=0.99,\n",
    "    xanchor=\"left\",\n",
    "    x=0.01,\n",
    "    title=\"Algorithm:\",\n",
    "    bordercolor=\"black\",\n",
    "    borderwidth=1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57307fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a series of boxplots to examine distributions of results\n",
    "for i in results_df.columns.drop([\"algo\",\"dataset_size\"]):\n",
    "    title=titles.get(i)\n",
    "    \n",
    "    fig = px.box(results_df,\n",
    "                 x=\"algo\",\n",
    "                 y=i,\n",
    "                 height=760,\n",
    "                 width=760)\n",
    "    \n",
    "    fig.update_xaxes(\n",
    "        showgrid=True,\n",
    "        showline=True,\n",
    "        linewidth=1,\n",
    "        mirror=True,\n",
    "        title=\"Algortihm\"\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        showgrid=True,\n",
    "        showline=True,\n",
    "        linewidth=1,\n",
    "        mirror=True,\n",
    "        title=f\"{title}\"\n",
    "        )\n",
    "\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

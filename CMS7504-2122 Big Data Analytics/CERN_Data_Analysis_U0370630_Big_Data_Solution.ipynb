{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of CERN Beam Position Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4] Big Data Analysis - Full Dataset [Spark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Machine Specifications:\n",
    "Processor: Intel-i9 10900K; clocked to 4.7 Ghz; watercooled\n",
    "Mobo: ASUS ROGSTRX Z490-F\n",
    "Memory: Corsair Vengeance 32GB 3200 MHz DDR4\n",
    "SSD:  Samsung 1TB 970 EVO\n",
    "GPU: EVGA NVIDIA GeForce RTX 3080 XC3 BLACK 10GB\n",
    "OS: Windows 10 [10.0.19042 Build 19042]\n",
    "\"\"\"\n",
    "\n",
    "#import required librarys for exploratory data analysis\n",
    "import os\n",
    "import numpy as np #version 1.19.2\n",
    "import pandas as pd #version 1.1.3\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"simple_white\"\n",
    "\n",
    "import findspark #version 1.4.2\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_LIST = os.listdir(\".\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_format_ingest(file_count):\n",
    "\n",
    "    #create a dataframe to store all the ingested data into\n",
    "    MASTER_INPUT=pd.DataFrame()\n",
    "    \n",
    "    for FILE in FILE_LIST[0:file_count]:\n",
    "        #print the active file for status check\n",
    "        print(f\"Reading file name...{FILE}\")\n",
    "        FILE_PATH=\"./data/{}\".format(FILE)\n",
    "        \n",
    "        #read the file\n",
    "        RAW_INPUT=pd.read_csv(FILE_PATH,delimiter=\",\")\n",
    "        \n",
    "        #extract the file name for the unique experiment log\n",
    "        RAW_INPUT[\"experiment\"]=FILE[11:34]\n",
    "        \n",
    "        #extract the axis for analysis\n",
    "        RAW_INPUT[\"axis\"]=FILE[34]\n",
    "        \n",
    "        #append each file to the master dataframe\n",
    "        MASTER_INPUT=MASTER_INPUT.append(RAW_INPUT)\n",
    "\n",
    "    #reset the index to give each row a unique index number\n",
    "    MASTER_INPUT=MASTER_INPUT.reset_index()\n",
    "    \n",
    "    #swap the old index column to be sensor number\n",
    "    MASTER_INPUT=MASTER_INPUT.rename({\"index\":\"sensor\"},axis=1)\n",
    " \n",
    "    #iterate the old columns for turns to a new column with \"turn\" in the column name\n",
    "    ORIGINAL_COL_NAMES = MASTER_INPUT.columns[np.arange(1,6601)]\n",
    "    NEW_COL_NAMES = [\"turn_\"+str(i) for i in np.arange(1,len(ORIGINAL_COL_NAMES)+1)]\n",
    "    COLUMN_MAPPER = dict(zip(ORIGINAL_COL_NAMES,NEW_COL_NAMES))\n",
    "    MASTER_INPUT=MASTER_INPUT.rename(columns=COLUMN_MAPPER)\n",
    "    \n",
    "    #use one-hot encoding to split out the axis column from string to numeric in case required\n",
    "    MASTER_INPUT=pd.get_dummies(MASTER_INPUT,columns=[\"axis\"])    \n",
    "    \n",
    "    #set the datatypes for ease of analysis\n",
    "    MASTER_INPUT=MASTER_INPUT.astype({\"axis_x\":float,\n",
    "                                      \"axis_y\":float,\n",
    "                                      \"sensor\":int\n",
    "                                     })\n",
    "    \n",
    "    return MASTER_INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#findspark used to locate spark folder\n",
    "findspark.init('C:/Users/carlw/spark-3.1.2-bin-hadoop3.2')\n",
    "\n",
    "#spark context created, with 8/10 cores assigned and memory increased from default to 16/32GB\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CERN_Analysis\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .config(\"spark.executor.memory\", \"16G\") \\\n",
    "    .config(\"spark.driver.memory\", \"16G\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4G\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\") #arrow enabled to quickly manage the pandas dataframe conversion to spark dataframe\n",
    "\n",
    "spark #allows access to the SparkUI to monitor activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#read in the full dataset\n",
    "full_dataset_df=wide_format_ingest(66)\n",
    "\n",
    "#print the dataset shape\n",
    "print(f\"Dataframe shape: {full_dataset_df.shape}\")\n",
    "\n",
    "#examine the memory usage\n",
    "full_dataset_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the pandas dataframe to a spark dataframe\n",
    "cern_dataset = spark.createDataFrame(full_dataset_df)\n",
    "\n",
    "#use an iterator to create a list of input columns for the vector assembler [6600 of them]\n",
    "input_cols_for_vector_assembler = [\"turn_\"+str(i) for i in np.arange(1,6601)]\n",
    "\n",
    "#create a vector assembler to take the 6600 readings per observation and create a single vector called \"features\"\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=input_cols_for_vector_assembler,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "#create an evaluator using silhouette analysis to assess how well the KMeans algo is performing\n",
    "evaluator = ClusteringEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    featuresCol=\"features\",\n",
    "    metricName=\"silhouette\",\n",
    "    distanceMeasure=\"squaredEuclidean\")\n",
    "\n",
    "#create a new column in the spark dataframe with \n",
    "assembled_dataset=assembler.transform(cern_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time.time() #start the timer\n",
    "KMeans_algo=KMeans(featuresCol=\"features\",k=5,seed=13) #create the model using 5 clusters and the \"features column\"\n",
    "KMeans_fit=KMeans_algo.fit(assembled_dataset) #fit the dataset using the KMeans model\n",
    "output=KMeans_fit.transform(assembled_dataset) #create a results dataframe for post processing\n",
    "score=evaluator.evaluate(output) #determine the silhouette score of the results\n",
    "t1=time.time() #stop the timer\n",
    "total_time=t1-t0 #calculate the time taken\n",
    "\n",
    "print(f\"Silhouette Score: {score:.3f}\") #print the silhouette score\n",
    "print(f\"Time to solve: {total_time:.1f}\") #print the time taken to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a results dataframe to store summary statistics from the spark results\n",
    "SPARK_RESULTS_DF=pd.DataFrame(columns=[\"cluster\", #cluster id\n",
    "                                       \"count\", #number of observations\n",
    "                                       \"unique_sensors\", #number of unique sensor ID's in the cluster\n",
    "                                       \"x_axis_count\", #number of x axis observations in the cluster\n",
    "                                       \"y_axis_count\", #number of y axis observations in the cluster\n",
    "                                       \"experiment_count\"]) #number of unique experiment ID's in the cluster\n",
    "\n",
    "for i in range(0,5):\n",
    "    #status message the computation is starting\n",
    "    print(f\"Analysing cluster: {i+1}\")\n",
    "    \n",
    "    #start time\n",
    "    t0=time.time()\n",
    "    \n",
    "    #count the number of observations in the cluster\n",
    "    count=output.filter(output.prediction==i).count()\n",
    "    \n",
    "    #count the number of unique sensor ID's in the cluster\n",
    "    unique_sensors=output.select(\"sensor\").filter(output.prediction==i).distinct().count()\n",
    "    \n",
    "    #count the number of x axis observations in the cluster\n",
    "    x_axis=output.select(\"axis_x\").filter(output.prediction==i).filter(output.axis_x==1).count()\n",
    "    \n",
    "    #count the number of y axis observations in the cluster\n",
    "    y_axis=output.select(\"axis_y\").filter(output.prediction==i).filter(output.axis_y==1).count()\n",
    "    \n",
    "    #count the number of unique experiment ID's in the cluster\n",
    "    experiment=output.select(\"experiment\").filter(output.prediction==i).distinct().count()\n",
    "    \n",
    "    #end time\n",
    "    t1=time.time()    \n",
    "    \n",
    "    #append the summary statistics to the summary dataframe\n",
    "    SPARK_RESULTS_DF=SPARK_RESULTS_DF.append({\n",
    "        \"cluster\":i+1,\n",
    "        \"count\":count,\n",
    "        \"unique_sensors\":unique_sensors,\n",
    "        \"x_axis_count\":x_axis,\n",
    "        \"y_axis_count\":y_axis,\n",
    "        \"experiment_count\":experiment\n",
    "    },ignore_index=True)\n",
    "    #status message that the computation is complete\n",
    "    print(f\"Time taken: {t1-t0:.1f}s\")\n",
    "    print(\"===============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_RESULTS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine the sensor number in cluster number 2 [cluster 1 in the 0-4 space]\n",
    "output.select(\"sensor\").filter(output.prediction==1).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine the sensor number in cluster number 4 [cluster 3 in the 0-4 space]\n",
    "output.select(\"sensor\").filter(output.prediction==3).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine which cluster sensor number 526 resides in\n",
    "output.select(\"prediction\").filter(output.sensor==526).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5] Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe to store summary statistics for silhouette vs. cluster number analysis\n",
    "SPARK_VALIDATION_DF=pd.DataFrame(columns=[\"cluster_no\",\"silhouette_score\",\"time\"])\n",
    "\n",
    "#list of clusters to examine as part of validation\n",
    "cluster_checks=[5,6,7]\n",
    "\n",
    "#short loop to iterate over the clusters listed and append to a summary dataframe\n",
    "for i in cluster_checks:\n",
    "    t0=time.time()\n",
    "    KMeans_algo_val=KMeans(featuresCol=\"features\",k=i,seed=13)\n",
    "    KMeans_fit=KMeans_algo_val.fit(assembled_dataset)\n",
    "    validation_output=KMeans_fit.transform(assembled_dataset)\n",
    "    score=evaluator.evaluate(validation_output)\n",
    "    t1=time.time()\n",
    "    total_time=t1-t0\n",
    "    SPARK_VALIDATION_DF=SPARK_VALIDATION_DF.append({\n",
    "        \"cluster_no\":i,\n",
    "        \"silhouette_score\":score,\n",
    "        \"time\":total_time,\n",
    "    },ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a scatter plot with two y-axes, one for silhouette and one for time taken\n",
    "\n",
    "validation_plot = make_subplots(specs=[[{\"secondary_y\":True}]])\n",
    "\n",
    "validation_plot.add_trace(\n",
    "    go.Scatter(\n",
    "        x=SPARK_VALIDATION_DF.cluster_no,\n",
    "        y=SPARK_VALIDATION_DF.silhouette_score,\n",
    "        name=\"Silhouette Score\"),\n",
    "        secondary_y=False)\n",
    "\n",
    "validation_plot.add_trace(\n",
    "    go.Scatter(\n",
    "        x=SPARK_VALIDATION_DF.cluster_no,\n",
    "        y=SPARK_VALIDATION_DF.time,\n",
    "        name=\"Time\"),\n",
    "        secondary_y=True)\n",
    "\n",
    "validation_plot.update_layout(\n",
    "    title_text=\"PySpark Validation:k=5 to 7\")\n",
    "\n",
    "validation_plot.update_xaxes(\n",
    "    title=\"k\")\n",
    "\n",
    "validation_plot.update_yaxes(\n",
    "    title=\"Silhouette Score [-]\",\n",
    "    secondary_y=False)\n",
    "\n",
    "validation_plot.update_yaxes(\n",
    "    title=\"Time [-]\",\n",
    "    secondary_y=True)\n",
    "\n",
    "validation_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop the spark context once all analysis is complete\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

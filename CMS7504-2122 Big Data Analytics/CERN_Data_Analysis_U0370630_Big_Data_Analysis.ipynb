{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of CERN Beam Position Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1] Data structure and statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset consists of: 33 individual experiments, each experiment has:\n",
    "1. 2 independent axes of measurement [x and y]\n",
    "2. 536 individual sensors per axis\n",
    "3. 6600 measurements per sensor\n",
    "Totalling 233,481,600 measurement points for analysis.    \n",
    "    \n",
    "Data is supplied as 66 comma seperated value files, one per axis and experiment.\n",
    "\n",
    "Missing information:\n",
    "1. Frequency - unclear on data frequency capture i.e. rotation/sensor capture rate.\n",
    "2. Threshold value for beam being detected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Machine Specifications:\n",
    "Processor: Intel-i9 10900K; clocked to 4.7 Ghz; watercooled\n",
    "Mobo: ASUS ROGSTRX Z490-F\n",
    "Memory: Corsair Vengeance 32GB 3200 MHz DDR4\n",
    "SSD:  Samsung 1TB 970 EVO\n",
    "GPU: EVGA NVIDIA GeForce RTX 3080 XC3 BLACK 10GB\n",
    "OS: Windows 10 [10.0.19042 Build 19042]\n",
    "\"\"\"\n",
    "\n",
    "#import required librarys for exploratory data analysis\n",
    "import os\n",
    "import numpy as np #version 1.19.2\n",
    "import pandas as pd #version 1.1.3\n",
    "import time\n",
    "from itertools import cycle\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"simple_white\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import SilhouetteVisualizer,KElbowVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of files constituting the dataset for easy access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_LIST = os.listdir(\".\\data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exmaine the file list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    print(f\"File {i+1}: {FILE_LIST[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files are structured in pairs, hence read the first and second files which are the x and y axis datasets for the first experiment dated 29May2018 at 17:48pm [and 36.697s].\n",
    "\n",
    "Data to be recorded in a Pandas DataFrame for interrogation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read_x = pd.read_csv(f\".\\data\\{FILE_LIST[0]}\",delimiter=\",\")\n",
    "data_read_y = pd.read_csv(f\".\\data\\{FILE_LIST[1]}\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the first DataFrame for the X axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is configured into records or measurements [time] across the columns and sensor numbers on the rows.\n",
    "\n",
    "Examine the min, max, mean, standard deviation and 25, 50 and 75% quartile values by event for first ten events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read_x.iloc[:,0:10].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the min, max, mean, standard deviation and 25, 50 and 75% quartile values by sensor for first ten sensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read_x.transpose().iloc[:,0:10].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The min, max and standard deviation values by sensor are much lower than the values by event; indicating that there is more variation sensor to sensor than there is event to event.\n",
    "\n",
    "The 25% and 75% quartiles by event appear to range between +/- 0.5 the maxium values appear to be below 9.\n",
    "\n",
    "Create a heatmap of the data at two ranges: +/-0.5 and +/-9.0 to visualise the initial observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap(input_data, lower_z, upper_z):\n",
    "    heatmap = px.imshow(\n",
    "                      input_data,\n",
    "                      labels=dict(x=\"Turn [count]\",\n",
    "                                  y=\"Sensor [number]\"),\n",
    "                      width=1080,height=1080,\n",
    "                      zmin=lower_z,zmax=upper_z\n",
    "                     )\n",
    "\n",
    "    heatmap.update_xaxes(dtick=500)\n",
    "    heatmap.update_yaxes(dtick=50)\n",
    "\n",
    "    heatmap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_heatmap(data_read_x,-0.5,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_heatmap(data_read_y,-0.5,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_heatmap(data_read_x,-9,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_heatmap(data_read_y,-9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find max sensor\n",
    "find min sensor\n",
    "plot distributions\n",
    "plot sensor reading vs. cycle for several close to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose the input data so the individual sensors are columns / features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_read_x_T_desc = data_read_x.transpose().describe()\n",
    "data_read_x_T_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a histogram of the means for each sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_x_sensor_means = px.histogram(data_read_x_T_desc.transpose()[[\"mean\"]],\n",
    "                                        log_y=True,\n",
    "                                        title=\"Distribution of Mean Sensor Readings\",\n",
    "                                        )\n",
    "histogram_x_sensor_means.update_xaxes(title=\"Sensor Reading [-]\",\n",
    "                  dtick=\"5\")\n",
    "\n",
    "histogram_x_sensor_means.update_yaxes(title=\"Count [-]\")\n",
    "\n",
    "histogram_x_sensor_means.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_x_sensor_std = px.histogram(data_read_x_T_desc.transpose()[[\"std\"]],\n",
    "                                        log_y=False,\n",
    "                                        title=\"Distribution of Sensor Reading Standard Deviations\",\n",
    "                                        )\n",
    "histogram_x_sensor_std.update_xaxes(title=\"Sensor Reading [-]\",\n",
    "                  dtick=\"0.1\")\n",
    "\n",
    "histogram_x_sensor_std.update_yaxes(title=\"Count [-]\")\n",
    "\n",
    "histogram_x_sensor_std.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a log scale for the y axis allows outling sensors to be identified.\n",
    "\n",
    "Transpose the descriptive statistics table so the statistical descriptors [mean, max, ...] are features / columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read_x_T_desc_T=data_read_x_T_desc.transpose()\n",
    "data_read_x_T_desc_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the minimum and maximum reading sensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_read_x_T_desc_T[data_read_x_T_desc_T[\"mean\"]==data_read_x_T_desc_T[\"mean\"].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_read_x_T_desc_T[data_read_x_T_desc_T[\"mean\"]==data_read_x_T_desc_T[\"mean\"].min()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the maximum and minimum reading sensors to examine the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_x_maximum_sensor = px.histogram(data_read_x.iloc[485,:],\n",
    "                                          log_y=True,\n",
    "                                          title=\"Distribution of Readings for Sensor 485\",\n",
    "                                          nbins=50\n",
    "                                          )\n",
    "histogram_x_maximum_sensor.update_xaxes(title=\"Sensor Reading [-]\",\n",
    "                  dtick=\"0.1\")\n",
    "\n",
    "histogram_x_maximum_sensor.update_yaxes(title=\"Count [-]\")\n",
    "\n",
    "histogram_x_maximum_sensor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_x_minimum_sensor = px.histogram(data_read_x.iloc[145,:],\n",
    "                                         log_y=True,\n",
    "                                         title=\"Distribution of Readings for Sensor 145\",\n",
    "                                         nbins=50\n",
    "                                         )\n",
    "histogram_x_minimum_sensor.update_xaxes(title=\"Sensor Reading [-]\",\n",
    "                  dtick=\"0.1\")\n",
    "\n",
    "histogram_x_minimum_sensor.update_yaxes(title=\"Count [-]\")\n",
    "\n",
    "histogram_x_minimum_sensor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_x_median_sensor = px.histogram(data_read_x.iloc[526,:],\n",
    "                                          log_y=True,\n",
    "                                          title=\"Distribution of Readings for Sensor 526\",\n",
    "                                          nbins=50\n",
    "                                          )\n",
    "histogram_x_median_sensor.update_xaxes(title=\"Sensor Reading [-]\",\n",
    "                  dtick=\"0.1\")\n",
    "\n",
    "histogram_x_median_sensor.update_yaxes(title=\"Count [-]\")\n",
    "\n",
    "histogram_x_median_sensor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_x_zero_sensor = px.histogram(data_read_x.iloc[0,:],\n",
    "                                          log_y=True,\n",
    "                                          title=\"Distribution of Readings for Sensor 0\",\n",
    "                                          nbins=50\n",
    "                                          )\n",
    "histogram_x_zero_sensor.update_xaxes(title=\"Sensor Reading [-]\",\n",
    "                  dtick=\"0.1\")\n",
    "\n",
    "histogram_x_zero_sensor.update_yaxes(title=\"Count [-]\")\n",
    "\n",
    "histogram_x_zero_sensor.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four unique distributions observed from four different sensors:\n",
    "1. Sensor 0 shows a very uniform distribution between -1.5 and 1.5. Potentially a mix of two normal distributions.\n",
    "2. Sensor 526 shows very little distribution, nearly all points at exactly zero with four individual readings slightly lower.\n",
    "3. Sensor 145 [minimum mean] \n",
    "\n",
    "Examine scatter plots of each sensor vs. rotation to observe if there are any paterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sensor_list=[0,145,485,526]\n",
    "\n",
    "for s in sensor_list:\n",
    "    line_chart = px.line(data_read_x.iloc[s,:],\n",
    "                                    title=f\"Line Plot for Sensor: {s}\")\n",
    "    line_chart.update_xaxes(range=[0,100],dtick=10)\n",
    "    line_chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Memory Efficiency Analysis for Wide vs. Long Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_format_ingest(file_count):\n",
    "\n",
    "    #create a dataframe to store all the ingested data into\n",
    "    MASTER_INPUT=pd.DataFrame()\n",
    "    \n",
    "    for FILE in FILE_LIST[0:file_count]:\n",
    "        #print the active file for status check\n",
    "        print(f\"Reading file name...{FILE}\")\n",
    "        FILE_PATH=\"./data/{}\".format(FILE)\n",
    "        \n",
    "        #read the file\n",
    "        RAW_INPUT=pd.read_csv(FILE_PATH,delimiter=\",\")\n",
    "        \n",
    "        #extract the file name for the unique experiment log\n",
    "        RAW_INPUT[\"experiment\"]=FILE[11:34]\n",
    "        \n",
    "        #extract the axis for analysis\n",
    "        RAW_INPUT[\"axis\"]=FILE[34]\n",
    "        \n",
    "        #append each file to the master dataframe\n",
    "        MASTER_INPUT=MASTER_INPUT.append(RAW_INPUT)\n",
    "\n",
    "    #reset the index to give each row a unique index number\n",
    "    MASTER_INPUT=MASTER_INPUT.reset_index()\n",
    "    \n",
    "    #swap the old index column to be sensor number\n",
    "    MASTER_INPUT=MASTER_INPUT.rename({\"index\":\"sensor\"},axis=1)\n",
    " \n",
    "    #iterate the old columns for turns to a new column with \"turn\" in the column name\n",
    "    ORIGINAL_COL_NAMES = MASTER_INPUT.columns[np.arange(1,6601)]\n",
    "    NEW_COL_NAMES = [\"turn_\"+str(i) for i in np.arange(1,len(ORIGINAL_COL_NAMES)+1)]\n",
    "    COLUMN_MAPPER = dict(zip(ORIGINAL_COL_NAMES,NEW_COL_NAMES))\n",
    "    MASTER_INPUT=MASTER_INPUT.rename(columns=COLUMN_MAPPER)\n",
    "    \n",
    "    #use one-hot encoding to split out the axis column from string to numeric in case required\n",
    "    MASTER_INPUT=pd.get_dummies(MASTER_INPUT,columns=[\"axis\"])    \n",
    "    \n",
    "    #set the datatypes for ease of analysis\n",
    "    MASTER_INPUT=MASTER_INPUT.astype({\"axis_x\":float,\n",
    "                                      \"axis_y\":float,\n",
    "                                      \"sensor\":int\n",
    "                                     })\n",
    "    \n",
    "    return MASTER_INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_format_ingest(maximum_file):\n",
    "    \n",
    "    MASTER_INPUT=pd.DataFrame()\n",
    "    \n",
    "    for FILE in FILE_LIST[0:maximum_file]:\n",
    "        print(f\"Reading file name...{FILE}\")\n",
    "        FILE_PATH=\"./data/{}\".format(FILE)\n",
    "        \n",
    "        #transpose the read dataframe so sensors become columns\n",
    "        RAW_INPUT=pd.read_csv(FILE_PATH,delimiter=\",\").transpose()\n",
    "        \n",
    "        #melt the dataframe into sensor number and readings\n",
    "        RAW_INPUT=pd.melt(RAW_INPUT,ignore_index=True,var_name=\"sensor\",value_name=\"reading\")\n",
    "        \n",
    "        #create a column to capture the turn number as a feature\n",
    "        TURNS=cycle(np.arange(0,6600,1))\n",
    "        RAW_INPUT[\"turn\"]=[next(TURNS) for TURN in range(len(RAW_INPUT))]\n",
    "        \n",
    "        RAW_INPUT[\"experiment\"]=FILE[11:34]        \n",
    "        RAW_INPUT[\"axis\"]=FILE[34]\n",
    "        MASTER_INPUT=MASTER_INPUT.append(RAW_INPUT)\n",
    "\n",
    "    #reset index and drop old index\n",
    "    MASTER_INPUT=MASTER_INPUT.reset_index().drop(\"index\",axis=1)\n",
    "    \n",
    "    MASTER_INPUT=pd.get_dummies(MASTER_INPUT,columns=[\"axis\"])\n",
    "    \n",
    "    MASTER_INPUT=MASTER_INPUT.astype({\"axis_x\":float,\n",
    "                                      \"axis_y\":float})\n",
    "    \n",
    "    return MASTER_INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_format_data_sample=wide_format_ingest(6)\n",
    "long_format_data_sample=long_format_ingest(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Wide format shape: {wide_format_data_sample.shape}\")\n",
    "print(f\"Long format shape: {long_format_data_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_format_data_sample.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_format_data_sample.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide format is significantly more memory efficint and will be used going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2] Prototype Algorithm Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially read in ~10% of the dataset to begin algorithm selection on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prototype_sample = wide_format_ingest(6)\n",
    "prototype_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split out the specific columns that will be used as features in the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_data=prototype_sample.iloc[:,1:6601]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a KMeans model to be used for Elbow analysis in selecting the optimum number of clusters. Random_state 13 has been selected for reproduceability and repeatability and will be used throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model_elbow = KMeans(random_state=13,algorithm=\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here \"YellowBrick\" [https://www.scikit-yb.org/en/latest/] for easy visualisation of the KMeans results.\n",
    "\n",
    "Initially this will be elbow analysis for k=3 to 12 and determining the optimium number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kelbow_visualizer=KElbowVisualizer(\n",
    "    kmeans_model_elbow,\n",
    "    k=(3,13)\n",
    "    )\n",
    "\n",
    "kelbow_visualizer.fit(modelling_data)\n",
    "\n",
    "kelbow_visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kelbow_visualizer_cal_har=KElbowVisualizer(\n",
    "    kmeans_model_elbow,\n",
    "    k=(3,13),\n",
    "    metric=\"calinski_harabasz\"\n",
    "    )\n",
    "\n",
    "kelbow_visualizer_cal_har.fit(modelling_data)\n",
    "\n",
    "kelbow_visualizer_cal_har.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow analysis indicates that k=5 will be the optimum; silhouette analysis completed on k=4, 5 and 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count=[4,5,6]\n",
    "\n",
    "for i in cluster_count:\n",
    "    kmeans_model = KMeans(n_clusters=i,algorithm=\"full\",random_state=13)\n",
    "    visualiser=SilhouetteVisualizer(kmeans_model,colors=\"yellowbrick\")\n",
    "    \n",
    "    t0=time.time()\n",
    "    visualiser.fit(modelling_data)    \n",
    "    t1=time.time()\n",
    "    total_time=t1-t0\n",
    "    visualiser.show()\n",
    "    print(f\"Time taken: {total_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette analysis also concludes that k=5 is the optimum number of clusters.\n",
    "\n",
    "In depth analysis of k=5 completed in order to understand the cluster distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=5,algorithm=\"full\",random_state=13)\n",
    "kmeans_model.fit(modelling_data)\n",
    "\n",
    "#store the cluster labels back in the master dataframe [adding 1 to each label to use 1-5 rather than 0-4]\n",
    "prototype_sample[\"cluster\"]=kmeans_model.labels_+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a histrogram of cluster vs. observations\n",
    "fig = px.histogram(prototype_sample,\n",
    "                   x=\"cluster\",\n",
    "                  log_y=True,\n",
    "                  title=f\"Cluster distribution for 5 clusters; across 3 experiments only\")\n",
    "fig.update_layout(bargap=0.1)\n",
    "fig.update_xaxes(dtick=1,title=\"Cluster [-]\")\n",
    "fig.update_yaxes(title=\"Count [-]\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters 2 and 3 appear to have the lowest count; next a summary table was created for each cluster capturing the number of observations, how many unique sensors were in the cluster, the number of observations that were in the X and Y axes, the number of unique experiment ID's and then the cluster mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a results dataframe for summary statistics\n",
    "RESULTS_DF=pd.DataFrame(columns=[\n",
    "    \"cluster_no\", #the cluster ID\n",
    "    \"count\", #the number of observations in the cluster\n",
    "    \"unique_sensors\", #the unique sensor count in the cluster\n",
    "    \"x_axis_count\", #the number of observations from the X axis\n",
    "    \"y_axis_count\", #the number of observations from the Y axis\n",
    "    \"experiment_count\", #the number of unique experiment ID's in the cluster\n",
    "    \"mean\", #the cluster mean\n",
    "    \"standard_deviation\" #the cluster standard deviation\n",
    "])\n",
    "\n",
    "for i in range(1,(prototype_sample.cluster.max()+1)):\n",
    "    #create a workingdata frame for the cluster being analysed\n",
    "    working_data=prototype_sample[prototype_sample[\"cluster\"]==i]\n",
    "    \n",
    "    #count the number of observations\n",
    "    count=len(working_data)\n",
    "    \n",
    "    #count the unique sensor observations\n",
    "    unique_sensors=len(working_data.sensor.unique())\n",
    "    \n",
    "    #count the number of observations from the X axis\n",
    "    x_axis_count=len(working_data[working_data[\"axis_x\"]==1])\n",
    "    \n",
    "    #count the number of observations from the Y axis\n",
    "    y_axis_count=len(working_data[working_data[\"axis_y\"]==1])\n",
    "    \n",
    "    #count the number of unique experiment ID's\n",
    "    experiment_count=len(working_data.experiment.unique())\n",
    "\n",
    "    #convert the cluster obervations [sensors and turns] to a numpy array\n",
    "    working_data=working_data.iloc[:,1:6601].to_numpy()\n",
    "    \n",
    "    #calcuate the mean of the numpy array\n",
    "    mean=working_data.mean()\n",
    "    \n",
    "    #calculate the standard deviation of the numpy array\n",
    "    std_dev=working_data.std()\n",
    "    \n",
    "    #append the results to the results summary dataframe\n",
    "    RESULTS_DF=RESULTS_DF.append({\n",
    "        \"cluster_no\":i,\n",
    "        \"count\":count,\n",
    "        \"unique_sensors\":unique_sensors,\n",
    "        \"x_axis_count\":x_axis_count,\n",
    "        \"y_axis_count\":y_axis_count,\n",
    "        \"experiment_count\":experiment_count,\n",
    "        \"mean\":mean,\n",
    "        \"standard_deviation\":std_dev\n",
    "    },ignore_index=True)\n",
    "    \n",
    "#set the data types for each column for readability\n",
    "RESULTS_DF=RESULTS_DF.astype({\n",
    "    \"cluster_no\":int,\n",
    "    \"count\":int,\n",
    "    \"unique_sensors\":int,\n",
    "    \"x_axis_count\":int,\n",
    "    \"y_axis_count\":int,\n",
    "    \"experiment_count\":int,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results dataframe shows that two of the clusters only contain one unique sensor, with three instances [one for each experiment] from the X-axis.\n",
    "\n",
    "Cluster 1 contains the bulk of the observations with a very event split between the x and y axis.\n",
    "\n",
    "The mean and standard deviation can be used to plot distributions, assuming a normal distribution, to visualise the clusters in the measurement space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.grid(b=True,which=\"major\")\n",
    "plt.xlabel(\"Sensor Measurement [-]\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "\n",
    "for i in RESULTS_DF.cluster_no.unique():\n",
    "    #retrieve the mean of the cluster being analysed\n",
    "    mu=RESULTS_DF[RESULTS_DF[\"cluster_no\"]==i].loc[:,\"mean\"]\n",
    "    \n",
    "    #retrieve the standard deviation of the cluster being analysed\n",
    "    sigma=RESULTS_DF[RESULTS_DF[\"cluster_no\"]==i].loc[:,\"standard_deviation\"]\n",
    "    \n",
    "    #create a series of 1000 equally spaces points on the x-axis to cover +/- standard deviations\n",
    "    x=np.linspace(mu-3*sigma,mu+3*sigma,1000)\n",
    "    \n",
    "    #using the x-values apply a normal distribution for the mean and standard deviation of the cluster\n",
    "    y=stats.norm.pdf(x,mu,sigma)\n",
    "    \n",
    "    #create a lable for the cluster being analysed\n",
    "    label=f\"Cluster {i:.0f}\"\n",
    "    \n",
    "    #plot the distribution for the cluster\n",
    "    plt.plot(x,y,label=label)\n",
    "\n",
    "#include the legend on the plot\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen there are five distinct clusters. There is likely some overlap between clusters 1, 4 and 5 which is reducing the Silhouette score - but only slightly. However, cluster 2 and 3 are very distinct from the rest.\n",
    "\n",
    "The next step is to evaluate this on a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3] Verification of prototype on ~20% of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "verification_sample = wide_format_ingest(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_modelling_data=verification_sample.iloc[:,1:6601]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_verification = KMeans(n_clusters=5,algorithm=\"full\",random_state=13)\n",
    "kmeans_verification.fit(verification_modelling_data)\n",
    "verification_sample[\"cluster\"]=kmeans_verification.labels_+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(verification_sample,\n",
    "                   x=\"cluster\",\n",
    "                  log_y=True,\n",
    "                  title=f\"Cluster distribution for 5 clusters; across 7 experiments only\")\n",
    "fig.update_layout(bargap=0.1)\n",
    "fig.update_xaxes(dtick=1,title=\"Cluster [-]\")\n",
    "fig.update_yaxes(title=\"Count [-]\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERIFICATION_DF=pd.DataFrame(columns=[\n",
    "    \"cluster_no\",\n",
    "    \"count\",\n",
    "    \"unique_sensors\",\n",
    "    \"x_axis_count\",\n",
    "    \"y_axis_count\",\n",
    "    \"experiment_count\",\n",
    "    \"mean\",\n",
    "    \"standard_deviation\"\n",
    "])\n",
    "\n",
    "for i in range(1,(verification_sample.cluster.max()+1)):\n",
    "    working_data=verification_sample[verification_sample[\"cluster\"]==i]\n",
    "    count=len(working_data)\n",
    "    unique_sensors=len(working_data.sensor.unique())\n",
    "    x_axis_count=len(working_data[working_data[\"axis_x\"]==1])\n",
    "    y_axis_count=len(working_data[working_data[\"axis_y\"]==1])\n",
    "    experiment_count=len(working_data.experiment.unique())\n",
    "\n",
    "    working_data=working_data.iloc[:,1:6601].to_numpy()\n",
    "    mean=working_data.mean()\n",
    "    std_dev=working_data.std()\n",
    "    \n",
    "    VERIFICATION_DF=VERIFICATION_DF.append({\n",
    "        \"cluster_no\":i,\n",
    "        \"count\":count,\n",
    "        \"unique_sensors\":unique_sensors,\n",
    "        \"x_axis_count\":x_axis_count,\n",
    "        \"y_axis_count\":y_axis_count,\n",
    "        \"experiment_count\":experiment_count,\n",
    "        \"mean\":mean,\n",
    "        \"standard_deviation\":std_dev\n",
    "    },ignore_index=True)\n",
    "    \n",
    "VERIFICATION_DF=VERIFICATION_DF.astype({\n",
    "    \"cluster_no\":int,\n",
    "    \"count\":int,\n",
    "    \"unique_sensors\":int,\n",
    "    \"x_axis_count\":int,\n",
    "    \"y_axis_count\":int,\n",
    "    \"experiment_count\":int,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.grid(b=True,which=\"major\")\n",
    "plt.xlabel(\"Sensor Measurement [-]\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "\n",
    "for i in VERIFICATION_DF.cluster_no.unique():\n",
    "    mu=VERIFICATION_DF[VERIFICATION_DF[\"cluster_no\"]==i].loc[:,\"mean\"]\n",
    "    sigma=VERIFICATION_DF[VERIFICATION_DF[\"cluster_no\"]==i].loc[:,\"standard_deviation\"]\n",
    "    x=np.linspace(mu-3*sigma,mu+3*sigma,1000)\n",
    "    y=stats.norm.pdf(x,mu,sigma)\n",
    "    label=f\"Cluster {i:.0f}\"\n",
    "    plt.plot(x,y,label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be see the distributions are still nicely distinct from each other, showing the model and hyperparameters have scaled up nicely to a slightly larger dataset."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
